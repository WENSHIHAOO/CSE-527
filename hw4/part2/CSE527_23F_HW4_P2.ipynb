{"cells":[{"cell_type":"markdown","metadata":{"id":"OZyBuMSqe542"},"source":["# CSE527 Homework4 - Part 2 (60 points)\n","**Due date: 23:59 on Dec 17, 2023**\n","\n","---\n","In this semester, we will use Google Colab for the assignments, which allows us to utilize resources that some of us might not have in their local machines such as GPUs. You will need to use your Stony Brook (*.stonybrook.edu) account for coding and Google Drive to save your results.\n","\n","## Google Colab Tutorial\n","---\n","Go to https://colab.research.google.com/notebooks/, you will see a tutorial named \"Welcome to Colaboratory\" file, where you can learn the basics of using google colab.\n","\n","Settings used for assignments: ***Edit -> Notebook Settings -> Runtime Type (Python 3)***.\n","\n","\n","## Local Machine Prerequisites\n","---\n","Since we are using Google Colab, all the code is run on the server environment where lots of libraries or packages have already been installed. In case of missing\n"," libraries or if you want to install them in your local machine, below are the links for installation.\n","* **Install Python 3.6**: https://www.python.org/downloads/ or use Anaconda (a Python distribution) at https://docs.continuum.io/anaconda/install/. Below are some materials and tutorials which you may find useful for learning Python if you are new to Python.\n","  - https://docs.python.org/3.6/tutorial/index.html\n","  - https://www.learnpython.org/\n","  - http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_tutorials.html\n","  - http://www.scipy-lectures.org/advanced/image_processing/index.html\n","\n","\n","* **Install Python packages**: install Python packages: `numpy`, `matplotlib`, `opencv-python` using pip, for example:\n","```\n","pip install numpy matplotlib opencv-python\n","```\n","\tNote that when using “pip install”, make sure that the version you are using is python3. Below are some commands to check which python version it uses in you machine. You can pick one to execute:\n","  \n","```  \n","    pip show pip\n","\n","    pip --version\n","\n","    pip -V\n","\n","```\n","\n","Incase of wrong version, use pip3 for python3 explictly.\n","\n","* **Install Jupyter Notebook**: follow the instructions at http://jupyter.org/install.html to install Jupyter Notebook and familiarize yourself  with it. *After you have installed Python and Jupyter Notebook, please open the notebook file 'HW1.ipynb' with your Jupyter Notebook and do your homework there.*\n","\n","## Description\n","---\n","In this homework you will experiment with SIFT features for scene matching and object recognition. You will work with the SIFT tutorial and code from the University of Toronto. In the compressed homework file, you will find the tutorial document (tutSIFT04.pdf) and a paper from the International Journal of Computer Vision (ijcv04.pdf) describing SIFT and object recognition. Although the tutorial document assumes matlab implemention, you should still be able to follow the technical details in it. In addition, you are **STRONGLY** encouraged to read this paper unless you’re already quite familiar with matching and recognition using SIFT.\n","\n","\n","## Using SIFT in OpenCV 3.x.x in Local Machine\n","---\n","Feature descriptors like SIFT and SURF are no longer included in OpenCV since version 3. This section provides instructions on how to use SIFT for those who use OpenCV 3.x.x. If you are using OpenCV 2.x.x then you are all set, please skip this section. Read this if you are curious about why SIFT is removed https://www.pyimagesearch.com/2015/07/16/where-did-sift-and-surf-go-in-opencv-3/.\n","\n","**We strongly recommend you to use SIFT methods in Colab for this homework**, the details will be described in the next section.\n","\n","However, if you want to use SIFT in your local machine, one simple way to use the OpenCV in-built function `SIFT` is to switch back to version 2.x.x, but if you want to keep using OpenCV 3.x.x, do the following:\n","1. uninstall your original OpenCV package\n","2. install opencv-contrib-python using pip (pip is a Python tool for installing packages written in Python), please find detailed instructions at https://pypi.python.org/pypi/opencv-contrib-python\n","\n","After you have your OpenCV set up, you should be able to use `cv2.xfeatures2d.SIFT_create()` to create a SIFT object, whose functions are listed at http://docs.opencv.org/3.0-beta/modules/xfeatures2d/doc/nonfree_features.html\n","\n","## Using SIFT in OpenCV 3.x.x in Colab (RECOMMENDED)\n","---\n","The default version of OpenCV in Colab is 4.8.0 as of Nov 2023. It also has opencv contrib installed and we can use SIFT method directly without any error. However this was not the case previously and students had to install opencv-contrib manually.  \n","\n"," You should be able to use use `cv2.xfeatures2d.SIFT_create()` to create a SIFT object, whose functions are listed at http://docs.opencv.org/3.0-beta/modules/xfeatures2d/doc/nonfree_features.html\n","\n","## Some Resources\n","---\n","In addition to the tutorial document, the following resources can definitely help you in this homework:\n","- http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_matcher/py_matcher.html\n","- http://docs.opencv.org/3.1.0/da/df5/tutorial_py_sift_intro.html\n","- http://docs.opencv.org/3.0-beta/modules/xfeatures2d/doc/nonfree_features.html?highlight=sift#cv2.SIFT\n","- http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"omyazyywtKQS"},"outputs":[],"source":["# Run this if you are using local (Not tested)\n","# pip install the OpenCV version from 'contrib'\n","# opencv-contrib-python==4.8.0.76\n","# opencv-python==4.8.0.76"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":197,"status":"ok","timestamp":1702424141000,"user":{"displayName":"Shihao Wen","userId":"04202530147146596133"},"user_tz":300},"id":"wnIVJsktWz5-","outputId":"d1a2bbf0-ceab-4e22-e7a2-be2e38e96564"},"outputs":[{"output_type":"stream","name":"stdout","text":["4.8.0\n"]}],"source":["# import packages here\n","import cv2\n","import math\n","import numpy as np\n","import matplotlib.pyplot as plt\n","print(cv2.__version__) # verify OpenCV version\n","import os"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":981,"status":"ok","timestamp":1702424144614,"user":{"displayName":"Shihao Wen","userId":"04202530147146596133"},"user_tz":300},"id":"DRJ7cx71t8XR","outputId":"ede2aa98-5d3f-4f45-e304-96206ab4303b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["# Mount your google drive where you've saved your assignment folder\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":134,"status":"ok","timestamp":1702424146939,"user":{"displayName":"Shihao Wen","userId":"04202530147146596133"},"user_tz":300},"id":"DYCDG18Ht50P","outputId":"f8679fda-ec16-4c16-b41a-4197c58ad625"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/CSE527/Wen_Shihao_113085521_hw4/part2\n"]}],"source":["# Replace '------' with the path such that \"CSE527-20S-HW2\" is your working directory\n","# cd '/content/gdrive/My Drive/------'\n","%cd '/content/gdrive/My Drive/CSE527/Wen_Shihao_113085521_hw4/part2'"]},{"cell_type":"markdown","metadata":{"id":"mROKmAhve545"},"source":["## Problem 1: Match transformed images using SIFT features\n","{10 points} You will transform a given image, and match it back to the original image using SIFT keypoints.\n","\n","- **Step 1 **. Use the function from SIFT class to detect keypoints from the given image. Plot the image with keypoints scale and orientation overlaid.\n","\n","- **Step 2 **. Rotate your image clockwise by 45 degrees with the `cv2.warpAffine` function. Extract SIFT keypoints for this rotated image and plot the rotated picture with keypoints scale and orientation overlaid just as in step 1.\n","\n","- **Step 3 **. Match the SIFT keypoints of the original image and the rotated imag using the `knnMatch` function in the `cv2.BFMatcher` class. Discard bad matches using the ratio test proposed by D.Lowe in the SIFT paper. Use **0.1** as the ratio in this homework. Note that this is for display purpose only. Draw the filtered good keypoint matches on the image and display it. The image you draw should have two images side by side with matching lines across them.\n","\n","- **Step 4 **. Use the RANSAC algorithm to find the affine transformation from the rotated image to the original image. You are not required to implement the RANSAC algorithm yourself, instead you could use the `cv2.findHomography` function (set the 3rd parameter `method` to `cv2.RANSAC`) to compute the transformation matrix. Transform the rotated image back using this matrix and the `cv2.warpPerspective` function. Display the recovered image.\n","\n","-  You might have noticed that the rotated image from step 2 is cropped. Try rotating the image without any cropping.\n","\n","Hints: In case of too many matches in the output image, use the ratio of 0.1 to filter matches.\n","\n","The image is a duplicate of *Table in front of window* by Pablo Picasso. See https://www.pablopicasso.org/ for more stories about Pablo Picasso and https://www.wikiart.org/en/pablo-picasso/table-in-front-of-window-1919 for more information about this work.\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1ON1ha0SFoGzkw7SzFVHneerLSWF-iLhI"},"executionInfo":{"elapsed":8951,"status":"ok","timestamp":1702424160721,"user":{"displayName":"Shihao Wen","userId":"04202530147146596133"},"user_tz":300},"id":"tn4AgU_lLp6_","outputId":"806fdd4d-67cd-45b6-916f-f2d7164e6857"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["def drawMatches(img1, kp1, img2, kp2, matches):\n","    \"\"\"\n","    My own implementation of cv2.drawMatches as OpenCV 2.4.9\n","    does not have this function available but it's supported in\n","    OpenCV 3.0.0\n","\n","    This function takes in two images with their associated\n","    keypoints, as well as a list of DMatch data structure (matches)\n","    that contains which keypoints matched in which images.\n","\n","    An image will be produced where a montage is shown with\n","    the first image followed by the second image beside it.\n","\n","    Keypoints are delineated with circles, while lines are connected\n","    between matching keypoints.\n","\n","    img1,img2 - Grayscale or Color images\n","    kp1,kp2 - Detected list of keypoints through any of the OpenCV keypoint\n","              detection algorithms\n","    matches - A list of matches of corresponding keypoints through any\n","              OpenCV keypoint matching algorithm\n","    \"\"\"\n","\n","    # Create a new output image that concatenates the two images together\n","    # (a.k.a) a montage\n","    rows1 = img1.shape[0]\n","    cols1 = img1.shape[1]\n","    rows2 = img2.shape[0]\n","    cols2 = img2.shape[1]\n","\n","    # Create the output image\n","    # The rows of the output are the largest between the two images\n","    # and the columns are simply the sum of the two together\n","    # The intent is to make this a colour image, so make this 3 channels\n","    out = np.zeros((max([rows1,rows2]),cols1+cols2,3), dtype='uint8')\n","\n","    # Place the first image to the left\n","    # stack if the inputs are gray images\n","    if len(img1.shape) == 2:\n","      img1 = np.dstack([img1, img1, img1])\n","    if len(img2.shape) == 2:\n","      img2 = np.dstack([img2, img2, img2])\n","\n","    out[:rows1,:cols1, :] = img1\n","\n","    # Place the next image to the right of it\n","    out[:rows2,cols1:, :] = img2\n","\n","    # For each pair of points we have between both images\n","    # draw circles, then connect a line between them\n","    for mat in matches:\n","        # Get the matching keypoints for each of the images\n","        img1_idx = mat.queryIdx\n","        img2_idx = mat.trainIdx\n","\n","        # x - columns\n","        # y - rows\n","        (x1,y1) = kp1[img1_idx].pt\n","        (x2,y2) = kp2[img2_idx].pt\n","\n","        # Draw a small circle at both co-ordinates\n","        # radius 4\n","        # colour blue\n","        # thickness = 1\n","        cv2.circle(out, (int(x1),int(y1)), 4, (255, 0, 0), 1)\n","        cv2.circle(out, (int(x2)+cols1,int(y2)), 4, (255, 0, 0), 1)\n","\n","        # Draw a line in between the two points\n","        # thickness = 1\n","        # colour blue\n","        cv2.line(out, (int(x1),int(y1)), (int(x2)+cols1,int(y2)), (0,255,0), 2)\n","    # Also return the image if you'd like a copy\n","    return out\n","\n","# Read image\n","img_input = cv2.imread('SourceImages/Picasso.png')\n","\n","##########--WRITE YOUR CODE HERE--##########\n","# initiate SIFT detector\n","sift = cv2.xfeatures2d.SIFT_create()\n","\n","# find the keypoints and descriptors with SIFT\n","kp, des = sift.detectAndCompute(img_input, None)\n","\n","# Draw keypoints on the image\n","# use cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS flag\n","res1 = cv2.drawKeypoints(img_input, kp, None, flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n","\n","# rotate image\n","# use cv2.warpAffine to rotate image\n","height, width = img_input.shape[:2]\n","img_center = (width/2, height/2)\n","RotationMatrix2D = cv2.getRotationMatrix2D(img_center, 45, 1.0)\n","\n","bound_h = int(height * abs(RotationMatrix2D[0,0]) + width * abs(RotationMatrix2D[0,1]))\n","bound_w = int(height * abs(RotationMatrix2D[0,1]) + width * abs(RotationMatrix2D[0,0]))\n","\n","RotationMatrix2D[1, 2] += (bound_h/2) - img_center[1]\n","RotationMatrix2D[0, 2] += (bound_w/2) - img_center[0]\n","\n","img_input_rot = cv2.warpAffine(img_input, RotationMatrix2D, (bound_w, bound_h))\n","\n","# find the keypoints and descriptors on the rotated image\n","sift_rot = cv2.xfeatures2d.SIFT_create()\n","kp_rot, des_rot = sift_rot.detectAndCompute(img_input_rot, None)\n","\n","# Draw keypoints on the rotated image\n","# use cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS flag\n","res2 = cv2.drawKeypoints(img_input_rot, kp_rot, None, flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n","##########-------END OF CODE-------##########\n","\n","# Plot result images\n","plt.figure(figsize=(14,8))\n","plt.subplot(1, 2, 1)\n","plt.imshow(cv2.cvtColor(res1, cv2.COLOR_BGR2RGB));\n","plt.title('original img')\n","plt.axis('off')\n","\n","plt.subplot(1, 2, 2)\n","plt.imshow(cv2.cvtColor(res2, cv2.COLOR_BGR2RGB));\n","plt.title('rotated img')\n","plt.axis('off')\n","\n","##########--WRITE YOUR CODE HERE--##########\n","# compute feature matching\n","# use the knnMatch function in the cv2.BFMatcher class\n","feature_matchs = cv2.BFMatcher().knnMatch(des, des_rot, k=2)\n","\n","# Apply ratio test to keep good matches; ratio=0.1\n","ratio = 0.1\n","good_matches = []\n","\n","for m in feature_matchs:\n","    if m[1].distance * ratio > m[0].distance:\n","       good_matches.append(m[0])\n","\n","# draw matching results with the given drawMatches function\n","res3 = drawMatches(img_input, kp, img_input_rot, kp_rot, good_matches)\n","##########-------END OF CODE-------##########\n","\n","plt.figure(figsize=(14,8))\n","plt.imshow(cv2.cvtColor(res3, cv2.COLOR_BGR2RGB));\n","plt.title('matching')\n","plt.axis('off')\n","\n","##########--WRITE YOUR CODE HERE--##########\n","# find perspective transform matrix using RANSAC\n","# use cv2.findHomography\n","src = np.float32([ kp[m.queryIdx].pt for m in good_matches])\n","des = np.float32([ kp_rot[m.trainIdx].pt for m in good_matches])\n","\n","homography, mask = cv2.findHomography(des, src, cv2.RANSAC)\n","# mapping rotataed image back with the calculated rotation matrix\n","# use cv2.warpPerspective\n","\n","res4 = cv2.warpPerspective(img_input_rot, homography, (width, height))\n","##########-------END OF CODE-------##########\n","\n","\n","# plot result images\n","plt.figure(figsize=(14,8));\n","plt.subplot(1, 2, 1);\n","plt.imshow(cv2.cvtColor(img_input, cv2.COLOR_BGR2RGB));\n","plt.title('original img');\n","plt.axis('off');\n","\n","plt.subplot(1, 2, 2);\n","plt.imshow(cv2.cvtColor(res4, cv2.COLOR_BGR2RGB));\n","plt.title('recovered img');\n","plt.axis('off');\n"]},{"cell_type":"markdown","metadata":{"id":"2-hGWnFUe548"},"source":["## Problem 2: Scene stitching with SIFT features\n","{20 points} You will match and align between different views of a scene with SIFT features.\n","\n","Use `cv2.copyMakeBorder` function to pad the center image with zeros into a larger size. Extract SIFT features for all images and go through the same procedures as you did in problem 1. Your goal is to find the affine transformation between the two images and then align one of your images to the other using `cv2.warpPerspective`. Use the `cv2.addWeighted` function (or your own implementation) to blend the aligned images and show the stitched result. Examples can be found at http://docs.opencv.org/trunk/d0/d86/tutorial_py_image_arithmetics.html.\n","Use parameters **0.5 and 0.5** for alpha blending.\n","\n","- **Step 1 (10points) **. Compute the transformation from the right image to the center image. Warp the right image with the computed transformation. Stitch the center and right images with alpha blending. Display the SIFT feature matching between the center and right images like you did in problem 1. Display the stitched result (center and right image).\n","\n","- **Step 2 (5 points)** Compute the transformation from the left image to the stitched image from step 1. Warp the left image with the computed transformation. Stich the left and result images from step 1 with alpha blending. Display the SIFT feature matching between the result image from step 1 and the left image like what you did in problem 1. Display the final stitched result (all three images).\n","\n","- **Laplacian (5 points)**. Instead of using `cv2.addWeighted` to do the blending, implement Laplacian Pyramids to blend the two aligned images. Tutorials can be found at http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_pyramids/py_pyramids.html. Display the stitched result (center and right image) and the final stitched result (all three images) with laplacian blending instead of alpha blending.\n","\n","Note that for the resultant stitched image, some might have different intensity in the overlapping and other regions, namely the overlapping region looks brighter or darker than others. To get full credit, the final image should have uniform illumination.\n","\n","Hints: You need to find the warping matrix between images with the same mechanism from problem 1. You will need as many reliable matches as possible to find a good homography so DO NOT use 0.1 here. A suggested value would be 0.75 in this case.\n","\n","When you warp the image with cv2.warpPerspective, an important trick is to pass in the correct parameters so that the warped image has the same size with the padded_center image. Once you have two images with the same size, find the overlapping part and do the blending.\n","\n","The images are the Stony Brook's Student Activities Center from https://www.youvisit.com/tour/panoramas/stonybrook/80175?id=405."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1VdwSM66wC-1LGJ_4HACe45Rcf1ZM_IDm"},"executionInfo":{"elapsed":37338,"status":"ok","timestamp":1702424202778,"user":{"displayName":"Shihao Wen","userId":"04202530147146596133"},"user_tz":300},"id":"K2HhJDJELxb9","outputId":"067ca19a-0491-4453-b57f-7a0e0a057831"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["imgCenter = cv2.imread('SourceImages/sac_center.png', cv2.IMREAD_COLOR)\n","imgRight  = cv2.imread('SourceImages/sac_r.png', cv2.IMREAD_COLOR)\n","imgLeft   = cv2.imread('SourceImages/sac_l.png', cv2.IMREAD_COLOR)\n","\n","# initalize the stitched image as the center image\n","imgCenter = cv2.copyMakeBorder(imgCenter,160,160,400,400,cv2.BORDER_CONSTANT)\n","print(imgLeft.shape)\n","print(imgCenter.shape)\n","print(imgRight.shape)\n","\n","# blend two images\n","def alpha_blend(img_A, img_B):\n","    # Implement alpha_blending, using 0.5 and 0.5 for alphas\n","    ##########--WRITE YOUR CODE HERE--##########\n","    blended = cv2.addWeighted(img_A,0.5,img_B,0.5,0)\n","    ##########-------END OF CODE-------##########\n","    return blended\n","\n","\n","def Laplacian_Blending(img_A, img_B, mask, num_levels=5):\n","    # Implement Laplacian_blending\n","    # assume mask is float32 [0,1], it has the same size to img_A and img_B\n","    # the mask indicates which parts of img_A or img_B are blended together\n","    # num_levels is the number of levels in the pyramid\n","    assert img_A.shape==img_B.shape\n","    assert img_A.shape==mask.shape\n","    ##########--WRITE YOUR CODE HERE--##########\n","    G = img_A.copy()\n","    gpA = [G]\n","    for i in range(num_levels):\n","      G = cv2.pyrDown(G)\n","      gpA.append(G)\n","\n","    G = img_B.copy()\n","    gpB = [G]\n","    for i in range(num_levels):\n","      G = cv2.pyrDown(G)\n","      gpB.append(G)\n","\n","    lpA = [gpA[num_levels]]\n","    for i in range(num_levels,0,-1):\n","      GE = cv2.pyrUp(gpA[i])\n","      L = cv2.subtract(gpA[i-1],GE)\n","      lpA.append(L)\n","\n","    lpB = [gpB[num_levels]]\n","    for i in range(num_levels,0,-1):\n","      GE = cv2.pyrUp(gpB[i])\n","      L = cv2.subtract(gpB[i-1],GE)\n","      lpB.append(L)\n","\n","    LS = []\n","    for la,lb in zip(lpA,lpB):\n","      rows,cols,dpt = la.shape\n","      ls = np.hstack((la[:,0:cols//2], lb[:,cols//2:]))\n","      LS.append(ls)\n","\n","    blended = LS[0]\n","    for i in range(1,num_levels+1):\n","      blended = cv2.pyrUp(blended)\n","      blended = cv2.add(blended, LS[i])\n","    ##########-------END OF CODE-------##########\n","    return blended\n","\n","def getTransform(img1, img2):\n","    ##########--WRITE YOUR CODE HERE--##########\n","    # compute sift descriptors\n","    sift = cv2.xfeatures2d.SIFT_create()\n","\n","    # find all matches\n","    kp_1, des_1 = sift.detectAndCompute(img1, None)\n","    kp_2, des_2 = sift.detectAndCompute(img2, None)\n","    feature_matchs = cv2.BFMatcher().knnMatch(des_1, des_2, k=2)\n","\n","    # apply ratio test, use ratio = 0.75\n","    ratio = 0.75\n","    good_matches = []\n","\n","    for m in feature_matchs:\n","        if m[1].distance * ratio > m[0].distance:\n","          good_matches.append(m[0])\n","\n","    # draw matches\n","    img_match = drawMatches(img1, kp_1, img2, kp_2, good_matches)\n","\n","    # find perspective transform matrix using RANSAC\n","    src = np.float32([ kp_1[m.queryIdx].pt for m in good_matches])\n","    des = np.float32([ kp_2[m.trainIdx].pt for m in good_matches])\n","\n","    H, mask = cv2.findHomography(des, src, cv2.RANSAC)\n","    ##########-------END OF CODE-------##########\n","    # H is the perspective transform matrix\n","    # img_match is the image returned by drawMatches\n","    return H, img_match\n","\n","\n","def perspective_warping_alpha_blending(imgCenter, imgLeft, imgRight):\n","    ##########--WRITE YOUR CODE HERE--##########\n","    # Get homography from right to center\n","    # img_match_cr is your first output\n","    # call getTransform to get the transformation from the right to the center image\n","    H_Right, img_match_cr = getTransform(imgCenter, imgRight)\n","\n","    # Blend center and right\n","    # stitched_cr is your second output, returned by alpha_blending\n","    # call alpha_blending\n","    imgRight = cv2.warpPerspective(imgRight, H_Right, (imgCenter.shape[1], imgCenter.shape[0]))\n","    stitched_cr = alpha_blend(imgRight, imgCenter)\n","\n","    # Get homography from left to stitched center_right\n","    # img_match_lcr is your third output\n","    # call getTransform to get the transformation from the left to stitched_cr\n","    H_Left, img_match_lcr = getTransform(stitched_cr, imgLeft)\n","\n","    # Blend left and center_right\n","    # stitched_lcr is your fourth output, returned by alpha_blending\n","    # call alpha_blending\n","    imgLeft = cv2.warpPerspective(imgLeft, H_Left, (stitched_cr.shape[1], stitched_cr.shape[0]))\n","    stitched_lcr = alpha_blend(imgLeft, stitched_cr)\n","    ##########-------END OF CODE-------##########\n","    return img_match_cr, stitched_cr, img_match_lcr, stitched_lcr\n","\n","def perspective_warping_laplacian_blending(imgCenter, imgLeft, imgRight):\n","    ##########--WRITE YOUR CODE HERE--##########\n","    # Get homography from right to center\n","    # call getTransform to get the transformation from the right to the center image\n","    H_Right, img_match_cr = getTransform(imgCenter, imgRight)\n","\n","    # Blend center and right\n","    # stitched_cr is your first bonus output, returned by Laplacian_blending\n","    # call Laplacian_blending\n","    imgRight = cv2.warpPerspective(imgRight, H_Right, (imgCenter.shape[1], imgCenter.shape[0]))\n","\n","    match_mask1 = np.hstack([np.ones([imgCenter.shape[0],imgCenter.shape[1]//2,imgCenter.shape[2]], np.float32),\n","                             np.zeros([imgRight.shape[0],imgRight.shape[1]//2,imgRight.shape[2]], np.float32)])\n","\n","    stitched_cr = Laplacian_Blending(imgCenter, imgRight, match_mask1, 5)\n","    # Get homography from left to stitched center_right\n","    # call getTransform to get the transformation from the left to stitched_cr\n","    H_Left, img_match_lcr = getTransform(stitched_cr, imgLeft)\n","\n","    # Blend left and center_right\n","    # stitched_lcr is your second bonus output, returned by Laplacian_blending\n","    # call Laplacian_blending\n","    imgLeft = cv2.warpPerspective(imgLeft, H_Left, (imgCenter.shape[1], imgCenter.shape[0]))\n","\n","    match_mask2 = np.hstack([np.zeros([imgLeft.shape[0],imgLeft.shape[1]//2,imgLeft.shape[2]], np.float32),\n","                             np.ones([stitched_cr.shape[0],stitched_cr.shape[1]//2,stitched_cr.shape[2]], np.float32)])\n","\n","    stitched_lcr = Laplacian_Blending(imgLeft, stitched_cr, match_mask2, 5)\n","    ##########-------END OF CODE-------##########\n","    return img_match_cr, stitched_cr, img_match_lcr, stitched_lcr\n","\n","\n","img_match_cr, stitched_cr, img_match_lcr, stitched_lcr = perspective_warping_alpha_blending(imgCenter, imgLeft, imgRight)\n","img_match_cr_lap, stitched_cr_lap, img_match_lcr_lap, stitched_lcr_lap = perspective_warping_laplacian_blending(imgCenter, imgLeft, imgRight)\n","\n","plt.figure(figsize=(15,30));\n","plt.subplot(4, 1, 1);\n","plt.imshow(cv2.cvtColor(img_match_cr, cv2.COLOR_BGR2RGB));\n","plt.title(\"center and right matches\");\n","plt.axis('off');\n","plt.subplot(4, 1, 2);\n","plt.imshow(cv2.cvtColor(stitched_cr, cv2.COLOR_BGR2RGB));\n","plt.title(\"center, right: stitched result\");\n","plt.axis('off');\n","plt.subplot(4, 1, 3);\n","plt.imshow(cv2.cvtColor(img_match_lcr, cv2.COLOR_BGR2RGB));\n","plt.title(\"left and center_right matches\");\n","plt.axis('off');\n","plt.subplot(4, 1, 4);\n","plt.imshow(cv2.cvtColor(stitched_lcr, cv2.COLOR_BGR2RGB));\n","plt.title(\"left, center, right: stitched result\");\n","plt.axis('off');\n","plt.show();\n","\n","plt.figure(figsize=(15,30));\n","plt.subplot(4, 1, 1);\n","plt.imshow(cv2.cvtColor(stitched_cr_lap, cv2.COLOR_BGR2RGB));\n","plt.title(\"Laplacian - center, right: stitched result\");\n","plt.axis('off');\n","plt.subplot(4, 1, 2);\n","plt.imshow(cv2.cvtColor(stitched_lcr_lap, cv2.COLOR_BGR2RGB));\n","plt.title(\"Laplacian - left, center, right: stitched result\");\n","plt.axis('off');\n"]},{"cell_type":"markdown","metadata":{"id":"gpJS18x1l4Ro"},"source":["#Stitching a set of images\n","\n","This is similar to above problem except that you will be stitching a collection of images, without any given order.\n","You will be given the numpy images in the variable ```image_collection``` and you will have to write your algorithm to stitch and display an image that should resemble below image:\n"," ![Example Image](https://drive.google.com/uc?id=1hF6uT-NmWAapnAKxJqKf644Lb6njFh-2)\n","\n","\n","Please save the output image as  `stitched.png`\n","\n"," For simplicity of this HW, you will be provided the center_image in ```center_image``` and all others will be at ```image_collection```. The final output will be constructed up on the base image which uses ```center_image``` padded with 1600 pixels each on the top and the bottom edges and 3200 pixels each on the left and the right edges.\n","\n","\n","Note:\n","1. You cannot use image stitching libraries availble on the internet. You will have to implement it on your own based on the methods you have alrady implemented above.\n","2. Try not to manually hardcode the order of images for stitching. (10pts)\n","3. If you plan to use any graph algorithm, you may use code from internet,but you must cite the URL/library.\n","4. Please **include comments** where ever possible describing your algorithm.\n","5. Clean your code before submission to be read by TA. Try not to use too many code blocks, it makes your code less readable.\n","6. You can use alpha blending for this problem\n","7. You may experiment on less resolution version of these images to save time. But your submission should be on original resolution images\n","8. If you come across an artifact caused by cv2's warping method, this graphing (https://www.desmos.com/calculator/cefcmi6pvn  made by Kalyan), may help you.\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":1148,"status":"ok","timestamp":1702424236695,"user":{"displayName":"Shihao Wen","userId":"04202530147146596133"},"user_tz":300},"id":"ssNssgTBKnmK"},"outputs":[],"source":["\n","import os\n","from tqdm import tqdm\n","import random\n","def load_sac_images():\n","    # DO NOT CHANGE THE ORDER IN THIS LIST\n","    filelist=['sac_bus.png', 'sac_rb.png', 'sac_libside.png', 'sac_rsky.png', 'sac_r.png', 'sac_sky.png', 'sac_cb.png', 'sac_l.png' ]\n","    return [cv2.imread(f'SourceImages/{f}') for f in filelist]\n","\n","center_image = cv2.imread(f'SourceImages/sac_center.png')\n","image_collection = load_sac_images()"]},{"cell_type":"markdown","metadata":{"id":"YhVGbWfXq1_D"},"source":["#################YOUR CODE STARTS HERE #####################\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RdUBdMKYawkY","executionInfo":{"status":"ok","timestamp":1702425054622,"user_tz":300,"elapsed":815223,"user":{"displayName":"Shihao Wen","userId":"04202530147146596133"}},"outputId":"e2f53b3f-f6e1-415e-9ee7-e5d23f54e538"},"outputs":[{"output_type":"stream","name":"stderr","text":["Selecting Best Image: 100%|██████████| 8/8 [01:22<00:00, 10.33s/it]\n","Selecting Best Image: 100%|██████████| 7/7 [02:28<00:00, 21.21s/it]\n","Selecting Best Image: 100%|██████████| 6/6 [02:04<00:00, 20.82s/it]\n","Selecting Best Image: 100%|██████████| 5/5 [01:27<00:00, 17.49s/it]\n","Selecting Best Image: 100%|██████████| 4/4 [01:16<00:00, 19.16s/it]\n","Selecting Best Image: 100%|██████████| 3/3 [01:00<00:00, 20.31s/it]\n","Selecting Best Image: 100%|██████████| 2/2 [00:50<00:00, 25.14s/it]\n","Selecting Best Image: 100%|██████████| 1/1 [00:24<00:00, 24.21s/it]\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":15}],"source":["def stitching(center_image, image_collection, min_matches=10):\n","    stitched_result = cv2.copyMakeBorder(center_image, 1600, 1600, 3200, 3200, cv2.BORDER_CONSTANT)\n","\n","    sift = cv2.xfeatures2d.SIFT_create()\n","    while len(image_collection) > 0:\n","        max_matches = 0\n","        best_matches = []\n","        best_image_index = None\n","\n","        kp1, des1 = sift.detectAndCompute(stitched_result, None)\n","        for i, image in enumerate(tqdm(image_collection, desc=\"Selecting Best Image\")):\n","            kp2, des2 = sift.detectAndCompute(image, None)\n","            feature_matches = cv2.BFMatcher().knnMatch(des1, des2, k=2)\n","\n","            # use ratio = 0.75\n","            ratio = 0.75\n","            good_matches = []\n","            for m in feature_matches:\n","                if m[1].distance * ratio > m[0].distance:\n","                  good_matches.append(m[0])\n","\n","            if len(good_matches) > max_matches:\n","                max_matches = len(good_matches)\n","                best_matches = good_matches\n","                best_image_index = i\n","\n","        # Find perspective transform matrix using RANSAC\n","        kp2, des2 = sift.detectAndCompute(image_collection[best_image_index], None)\n","        src_pts = np.float32([kp1[m.queryIdx].pt for m in best_matches])\n","        dst_pts = np.float32([kp2[m.trainIdx].pt for m in best_matches])\n","        H, _ = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC)\n","\n","        # Warp the best image\n","        warped_image = cv2.warpPerspective(image_collection[best_image_index], H, (stitched_result.shape[1], stitched_result.shape[0]))\n","\n","        # Create masks to identify non-overlapping regions\n","        non_overlap_mask_warped = (stitched_result == 0) & (warped_image > 0)\n","\n","        # Blend the images\n","        stitched_result = cv2.addWeighted(stitched_result, 1, warped_image * non_overlap_mask_warped, 1, 0)\n","        del image_collection[best_image_index]\n","\n","    return stitched_result\n","\n","center_image = cv2.imread(f'SourceImages/sac_center.png')\n","image_collection = load_sac_images()\n","\n","stitched_result = stitching(center_image, image_collection)\n","cv2.imwrite('stitched.png', stitched_result)"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"U_FmkJhWe55I"},"source":["## Submission guidelines\n","---\n","\n","Plagiarism: plagiarism is strictly forbidden.   \n","Note: Please be advised that uploading your homework assignments to public platforms, such as GitHub, is STRICTLY PROHIBITED. Sharing your homework solutions in this manner (even after the course completion) constitutes a violation of academic integrity and will be treated as such.\n","\n"]}],"metadata":{"anaconda-cloud":{},"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":0}